# spark核心编程

Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：

Ø RDD : 弹性分布式数据集

Ø 累加器：分布式共享只写变量

Ø 广播变量：分布式共享只读变量

接下来我们一起看看这三大数据结构是如何在数据处理中使用的。

## 一 RDD

### 1.什么是RDD

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200907210408911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNzI3MDk1,size_16,color_FFFFFF,t_70#pic_center)

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的一种数据处理的模型。

代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。

RDD 是 Spark 提供的最重要的抽象概念，它是一种有容错机制的特殊数据集合，可以分布在集群的结点上，以函数式操作集合的方式进行各种并行操作。

本质上是一个只读的分区记录集合。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同结点上，从而可以在集群中的不同结点上进行并行计算

- 弹性
  - 存储的弹性：计算过程中内存不够时，它会和磁盘进行数据交换（内存与磁盘的自动切换）
  - 容错的弹性：数据丢失可以自动恢复；
  - 计算的弹性：计算出错重试机制；
  - 分片的弹性：可以根据数据的变化，多次改变分区的数量，已最后一次提交的job分区为结果；
- 分布式（可分区）：每个 RDD 可以分成多个分区，每个分区就是一个数据集片段，一个 RDD 的不同分区可以保存到集群中的不同结点上，从而可以在集群中的不同结点上进行并行计算
- 数据集：RDD封装了计算逻辑，并不保存数据
- 数据抽象：RDD是一个抽象类，需要子类具体实现
- 不可变：RDD封装的是计算逻辑（不是数据），每个RDD一旦生成，是不可以改变的，若要对其修改逻辑，就会产生新的RDD

小结：一个 RDD 可以简单的理解为一个分布式的元素集合，

每个 RDD 可以分成多个分区，每个分区可以在集群中不同的节点上进行计算；

RDD 表示只读的数据集，对 RDD 进行改动，只能通过 RDD 的转换操作，得到新的 RDD，并不会对原本的 RDD 有任何影响；

在 Spark 中，所有的工作都是以操作 RDD 为主，要么是创建 RDD，要么是转换已经存在 RDD 成为新的 RDD，要么在 RDD 上去执行一些操作来得到一些计算结果；

### 2.核心属性

![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110223959.png)
![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110224003.png)

**分区列表**

RDD中的分区，即数据集的基本组成单位，每个分区可以运行在不同节点上；

对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度，是实现分布式计算的重要属性。
![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110224326.png)

**分区计算函数**

一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，相同的计算逻辑应用在不同分区中（即每个分区的计算逻辑是相同的）；

每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果
![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110224330.png)

**RDD之间的依赖关系**

当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系；

RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。

在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。
![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110224340.png)

**分区器（可选）**

RDD的分区函数，即对数据的分区规则。主要有两种分区函数，一个是`基于哈希的HashPartitioner（默认）`，另外一个是`基于范围的RangePartitioner`。

只有key-value类型的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。

Partitioner函数不但决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出时的分区数量。
![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110224334.png)

**首选位置（可选）**

计算数据的位置 （本地化级别），可以设置数据读取的偏好位置，用来将Task发送给指定的节点，就近原则（节省网络IO）

对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。

按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。
![在这里插入图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110224336.png)

### 3.执行原理与依赖关系深入

**1、RDD依赖关系的本质内幕**

　　由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；在spark中，RDD之间存在两种类型的依赖关系：窄依赖(Narrow Dependency)和宽依赖(Wide Dependency 或者是 Narrow Dependency)；如图1所示显示了RDD之间的依赖关系。

　　　　　　　　　　　　![img](https://gitee.com/zisuu/picture/raw/master/img/20210111125233.png)

　　　　　　　　　　　　　　　　　　　　　　　　　　　　图1

 

从图1中可知：

　　**窄依赖**是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；

　　**宽依赖**是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；

　　**需要特别说明的是对join操作有两种情况**：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)；其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。

　　**总结**：在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。

　　**一对固定个数的窄依赖的理解**：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。

 

 

**2、****依赖关系下的数据流视图**

　　![img](https://gitee.com/zisuu/picture/raw/master/img/20210111125233.png)

　　

　　在spark中，会根据RDD之间的依赖关系将DAG图划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。

　　因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。

　　在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask；

　　简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说图2中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。

　　需要补充说明的是，在前面的课程中，我们实际动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。

 　![img](https://images2015.cnblogs.com/blog/855959/201610/855959-20161009115917862-1919710551.png)

![img](https://images2015.cnblogs.com/blog/855959/201610/855959-20161009120124230-1553441050.png)

 

 　

 

**3、Stage中任务执行的内幕思考**

　　在一个stage内部，从表面上看是数据在不断流动，然后经过相应的算子处理后再流向下一个算子，但实质是算子在流动；我们可以从如下两个方面来理解：

　　(1) 数据不动代码动；这点从算法构建和逻辑上来说，是算子作用于数据上，而算子处理数据一般有多个步骤，所以这里说数据不动代码动；

　　(2) 在一个stage内部，算子之所以会流动(pipeline)首先是因为算子合并，也就是所谓的函数式编程在执行的时候最终进行函数的展开，从而把一个stage内部的多个算子合并成为一个大算子(其内部包含了当前stage中所有算子对数据的所有计算逻辑)；其次是由于Transformation操作的Lazy特性。因为这些转换操作是Lazy的，所以才可以将这些算子合并；如果我们直接使用scala语言是不可以的，即使可以在算子前面加上一个Lazy关键字，但是它每次操作的时候都会产生中间结果。同时在具体算子交给集群的executor计算之前首先会通过Spark Framework（DAGScheduler）进行算子的优化（即基于数据本地性的pipeline）。

 ![img](https://images2015.cnblogs.com/blog/855959/201610/855959-20161009120237985-1054033906.png)

## 二RDD核心编程



### 1.RDD创建

```
    //常用创建RDD的三种方法
    val rdd1: RDD[Int] = sc.makeRDD(Array(1,2,3))
    val rdd2: RDD[Int] = sc.parallelize(Array(1,2,3))
    val rdd3: RDD[String] = sc.textFile("local path或者HDFS path")
```

### 2.RDD并行度与分区

默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能
够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。记住，这里
的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。

**1、从外部存储（文件）创建RDD**

从文件中创建RDD,可以指定分区数

```Scala
例如：local模式下
val lineRDD: RDD[String] = sc.textFile("./aa,txt")
 
默认并行数：def defaultMinPartitions: Int = math.min(totalCores, 2)
totalCores：任务运行的总核数
 
 
分区数有两种情况：
a，从本地文件file:///生成的rdd，操作时如果没有指定分区数，则默认分区数规则为：
（按照官网的描述，本地file的分片规则，应该按照hdfs的block大小划分，但实测的结果是固定按照32M来分片，可能是bug，不过不影响使用，因为spark能用所有hadoop接口支持的存储系统，所以spark textFile使用hadoop接口访问本地文件时和访问hdfs还是有区别的）
 
rdd的分区数 = max（本地file的分片数， sc.defaultMinPartitions）
 
b，从hdfs分布式文件系统hdfs://生成的rdd，操作时如果没有指定分区数，则默认分区数规则为：
rdd的分区数 = max（hdfs文件的block数目， sc.defaultMinPartitions）
```

 **源码如下：**

 ![img](https://gitee.com/zisuu/picture/raw/master/img/20210111103047.png)

**2、从集合（内存）中创建RDD**

内存中创建,可以指定并行度

```Scala
例如：Yarn、Standalone模式下
val valueRDD: RDD[String] = sc.makeRDD(Array("a", "b", "c", "d"))
默认并行度：def defaultParallelism: Int = math.max(totalCoreCount.get(), 2))
totalCoreCount：任务运行的总核数
 
分区数：
这种方式下，如果在parallelize操作时没有指定分区数，则
rdd的分区数 = sc.defaultParallelism
```

![img](https://gitee.com/zisuu/picture/raw/master/img/20210111103047.png)

### 3.RDD基本转换算子

**1.map(func)：**数据集中的每个元素经过用户自定义的函数转换形成一个新的RDD，新的RDD叫MappedRDD

（例1）

```
object Map {
  def main(args: Array[String]) {
    val conf = new SparkConf().setMaster("local").setAppName("map")
    val sc = new SparkContext(conf)
    val rdd = sc.parallelize(1 to 10)  //创建RDD
    val map = rdd.map(_*2)             //对RDD中的每个元素都乘于2
    map.foreach(x => print(x+" "))
    sc.stop()
  }
}
```

输出：

```
2 4 6 8 10 12 14 16 18 20
```

**(RDD依赖图：红色块表示一个RDD区，黑色块表示该分区集合，下同)**

![img](https://gitee.com/zisuu/picture/raw/master/img/20210111104459.png)

 

**2.flatMap(func):**与map类似，但每个元素输入项都可以被映射到0个或多个的输出项，最终将结果”扁平化“后输出

（例2）

```
   val rdd = sc.parallelize(1 to 5)
   val fm = rdd.flatMap(x => (1 to x)).collect()
   fm.foreach( x => print(x + " "))
```

输出：

```
1 1 2 1 2 3 1 2 3 4 1 2 3 4 5
```

如果是map函数其输出如下：

```
Range(1) Range(1, 2) Range(1, 2, 3) Range(1, 2, 3, 4) Range(1, 2, 3, 4, 5)
```

 **(RDD依赖图)**

![img](https://gitee.com/zisuu/picture/raw/master/img/20210111104557.png)





**3.mapPartitions(func):**类似与map，map作用于每个分区的每个元素，但mapPartitions作用于每个分区工

func的类型：Iterator[T] => Iterator[U]

假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,当在映射的过程中不断的创建对象时就可以使用mapPartitions比map的效率要高很多，比如当向数据库写入数据时，如果使用map就需要为每个元素创建connection对象，但使用mapPartitions的话就需要为每个分区创建connetcion对象

(例3)：输出有女性的名字：

```
object MapPartitions {
//定义函数
  def partitionsFun(/*index : Int,*/iter : Iterator[(String,String)]) : Iterator[String] = {
    var woman = List[String]()
    while (iter.hasNext){
      val next = iter.next()
      next match {
        case (_,"female") => woman = "["+index+"]"+next._1 :: woman
        case _ =>
      }
    }
    return  woman.iterator
  }
 
  def main(args: Array[String]) {
    val conf = new SparkConf().setMaster("local").setAppName("mappartitions")
    val sc = new SparkContext(conf)
    val l = List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female"))
    val rdd = sc.parallelize(l,2)
    val mp = rdd.mapPartitions(partitionsFun)
    /*val mp = rdd.mapPartitionsWithIndex(partitionsFun)*/
    mp.collect.foreach(x => (print(x +" ")))   //将分区中的元素转换成Aarray再输出
  }
}
```

输出：

```
kpop lucy
```

其实这个效果可以用一条语句完成

```
val mp = rdd.mapPartitions(x => x.filter(_._2 == ``"female"``)).map(x => x._1)　
```

之所以不那么做是为了演示函数的定义



 ***\*(RDD依赖图)\****

**![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410014739172-1332748150.png)**

 

**4.mapPartitionsWithIndex(func):**与mapPartitions类似，不同的时函数多了个分区索引的参数

func类型：(Int, Iterator[T]) => Iterator[U]

（例4）：将例3橙色的注释部分去掉即是

输出：（带了分区索引）

```
[0]kpop [1]lucy
```

 

**5.sample(withReplacement,fraction,seed):**以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样

(例5)：从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）

```
//省略
    val rdd = sc.parallelize(1 to 10)
    val sample1 = rdd.sample(true,0.5,3)
    sample1.collect.foreach(x => print(x + " "))
    sc.stop
```

 

**6.union(ortherDataset):**将两个RDD中的数据集进行合并，最终返回两个RDD的并集，若RDD中存在相同的元素也不会去重

```
//省略sc
   val rdd1 = sc.parallelize(1 to 3)
   val rdd2 = sc.parallelize(3 to 5)
   val unionRDD = rdd1.union(rdd2)
   unionRDD.collect.foreach(x => print(x + " "))
   sc.stop　
```

输出：

```
1 2 3 3 4 5
```

　　

**7.intersection(otherDataset):**返回两个RDD的交集

```
//省略sc
val rdd1 = sc.parallelize(1 to 3)
val rdd2 = sc.parallelize(3 to 5)
val unionRDD = rdd1.intersection(rdd2)
unionRDD.collect.foreach(x => print(x + " "))
sc.stop　
```

输出：

```
3 4
```

 

**8.distinct([numTasks]):**对RDD中的元素进行去重

```
//省略sc
val list = List(1,1,2,5,2,9,6,1)
val distinctRDD = sc.parallelize(list)
val unionRDD = distinctRDD.distinct()
unionRDD.collect.foreach(x => print(x + " "))　　
```

输出：

```
1 6 9 5 2
```

 

**9.cartesian(otherDataset):**对两个RDD中的所有元素进行笛卡尔积操作

```
//省略
val rdd1 = sc.parallelize(1 to 3)
val rdd2 = sc.parallelize(2 to 5)
val cartesianRDD = rdd1.cartesian(rdd2)
cartesianRDD.foreach(x => println(x + " "))
```

输出：

```
(1,2)
(1,3)
(1,4)
(1,5)
(2,2)
(2,3)
(2,4)
(2,5)
(3,2)
(3,3)
(3,4)
(3,5)
```



 **(RDD依赖图)**

 ![img](https://gitee.com/zisuu/picture/raw/master/img/20210111111441.png)

 

**10.coalesce(numPartitions，shuffle):**

 **小分区合并问题介绍**

在使用Spark进行数据处理的过程中，常常会使用filter方法来对数据进行一些预处理，过滤掉一些不符合条件的数据。在使用该方法对数据进行频繁过滤或者是过滤掉的数据量过大的情况下就会造成大量小分区的生成。在Spark内部会对每一个分区分配一个task执行，如果task过多，那么每个task处理的数据量很小，就会造成线程频繁的在task之间切换，使得资源开销较大，且很多任务等待执行，并行度不高，这会造成集群工作效益低下。

**为了解决这一个问题，常采用RDD中重分区的函数（coalesce函数或rePartition函数）来进行数据紧缩，减少分区数量，将小分区合并为大分区，从而提高效率**

先介绍下**宽依赖（发生shuffle）**和**窄依赖（不发生shuffle）**

- 窄依赖：父Rdd的分区最多只能被一个子Rdd的分区所引用，即一个父Rdd的分区对应一个子Rdd的分区，或者多个父Rdd的分区对应一个子Rdd的分区。即**一对一**或**多对一**，如下图左边所示。
- 宽依赖：RDD的分区依赖于父RDD的多个分区或所有分区，即存在一个父RDD的一个分区对应一个子RDD的多个分区。1个父RDD分区对应多个子RDD分区，这其中又分两种情况：1个父RDD对应所有子RDD分区（未经协同划分的Join）或者1个父RDD对应非全部的多个RDD分区（如groupByKey）。即**一对多**

![这里写图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210111123301.jpeg)

**Coalesce()方法和rePartition()方法**

Coalesce()方法源码：

```
def coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null)
    : RDD[T] = withScope {
  if (shuffle) {
    /** Distributes elements evenly across output partitions, starting from a random partition. */
    val distributePartition = (index: Int, items: Iterator[T]) => {
      var position = (new Random(index)).nextInt(numPartitions)
      items.map { t =>
        // Note that the hash code of the key will just be the key itself. The HashPartitioner
        // will mod it with the number of total partitions.
        position = position + 1
        (position, t)
      }
    } : Iterator[(Int, T)]

    // include a shuffle step so that our upstream tasks are still distributed
    new CoalescedRDD(
      new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),
      new HashPartitioner(numPartitions)),
      numPartitions).values
  } else {
    new CoalescedRDD(this, numPartitions)
  }
}1234567891011121314151617181920212223
```

rePartition()方法源码：

```
  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }123
```

通过源码可以看出**两者的区别**：**coalesce()方法的参数shuffle默认设置为false，repartition()方法就是coalesce()方法shuffle为true的情况。**

**使用情景**

假设RDD有N个分区，需要重新划分成M个分区：

1. N < M: 一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。**因为重分区前后相当于宽依赖，会发生shuffle过程**，此时可以使用coalesce(shuffle=true)，或者直接使用repartition()。
2. 如果N > M并且N和M相差不多(假如N是1000，M是100): 那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这是**前后是窄依赖关系**，可以使用coalesce(shuffle=false)。
3. 如果 N> M并且两者相差悬殊: 这时如果将shuffle设置为false，父子ＲＤＤ是窄依赖关系，他们同处在一个Ｓｔａｇｅ中，就可能造成spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以将shuffle设置为true。

**总结**

**如果传入的参数大于现有的分区数目，而shuffle为false，RDD的分区数不变，也就是说不经过shuffle，是无法将RDDde分区数变多的。**

**11.glom():**将RDD的每个分区中的类型为T的元素转换换数组Array[T]

```
//省略
val rdd = sc.parallelize(1 to 16,4)
val glomRDD = rdd.glom() //RDD[Array[T]]
glomRDD.foreach(rdd => println(rdd.getClass.getSimpleName))
sc.stop　
输出：
```

输出：

```
int[] //说明RDD中的元素被转换成数组Array[Int]
```

 ![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410015053218-1975193443.png)



 

**13.randomSplit(weight:Array[Double],seed):**根据weight权重值将一个RDD划分成多个RDD,权重越高划分得到的元素较多的几率就越大

```
//省略sc
val rdd = sc.parallelize(1 to 10)
val randomSplitRDD = rdd.randomSplit(Array(1.0,2.0,7.0))
randomSplitRDD(0).foreach(x => print(x +" "))
randomSplitRDD(1).foreach(x => print(x +" "))
randomSplitRDD(2).foreach(x => print(x +" "))
sc.stop　
```

输出：

```
2 4
3 8 9
1 5 6 7 10
```

### 5.RDD K-V算子

**1.mapValus(fun):**对[K,V]型数据中的V值map操作

**(例1)：对每个的的年龄加2**

```
object MapValues {
  def main(args: Array[String]) {
    val conf = new SparkConf().setMaster("local").setAppName("map")
    val sc = new SparkContext(conf)
    val list = List(("mobin",22),("kpop",20),("lufei",23))
    val rdd = sc.parallelize(list)
    val mapValuesRDD = rdd.mapValues(_+2)
    mapValuesRDD.foreach(println)
  }
}
```

输出：

```
(mobin,24)
(kpop,22)
(lufei,25)
```

**(RDD依赖图：红色块表示一个RDD区，黑色块表示该分区集合，下同)**

![image-20210111202247873](https://gitee.com/zisuu/picture/raw/master/img/20210111202247.png)

 

**2.flatMapValues(fun)：**对[K,V]型数据中的V值flatmap操作

**(例2):**

```
//省略<br>val list = List(("mobin",22),("kpop",20),("lufei",23))
val rdd = sc.parallelize(list)
val mapValuesRDD = rdd.flatMapValues(x => Seq(x,"male"))
mapValuesRDD.foreach(println)
```

输出：

```
(mobin,22)
(mobin,male)
(kpop,20)
(kpop,male)
(lufei,23)
(lufei,male)
```

如果是mapValues会输出：

```
(mobin,List(22, male))
(kpop,List(20, male))
(lufei,List(23, male))
```

（RDD依赖图）

**3.reduceByKey**

![image-20210111203058211](https://gitee.com/zisuu/picture/raw/master/img/20210111203058.png)

可以将数据按照相同的Key 对Value 进行聚合

```
    //list
    val list = List(("mobin",22),("kpop",20),("lufei",23),("lufei",25))
    val rdd = sc.parallelize(list)
    val reduceRdd= rdd.reduceByKey((x,y)=>x+y)
    reduceRdd.collect().foreach(println)
```

**4.groupByKey**

根据相同key对value进行分组

![image-20210111203339951](https://gitee.com/zisuu/picture/raw/master/img/20210111203340.png)

**和reduceByKey的区别:**

**shuffle的角度**：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是reduceByKey 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。

**从功能的角度**：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey

**5.aggregateByKey**

![image-20210111204530673](https://gitee.com/zisuu/picture/raw/master/img/20210111204530.png)

```
// TODO : 取出每个分区内相同 key 的最大值然后分区间相加
// aggregateByKey 算子是函数柯里化，存在两个参数列表
// 1. 第一个参数列表中的参数表示初始值
// 2. 第二个参数列表中含有两个参数
//	2.1 第一个参数表示分区内的计算规则
//	2.2 第二个参数表示分区间的计算规则

//list
val list = List(("a",1),("a",2),("b",3),("c",4))
val rdd = sc.parallelize(list)
val aggRdd = rdd.aggregateByKey(0)((x,y)=>math.max(x,y),(x,y)=>x+y)
aggRdd.collect().foreach(println)
    
(a,3)
(b,3)
(c,4)

```

**6.foldByKey**

![image-20210111204819049](https://gitee.com/zisuu/picture/raw/master/img/20210111204819.png)

![image-20210111204951650](https://gitee.com/zisuu/picture/raw/master/img/20210111204951.png)

**7.combineByKey**

combineByKey是spark中一个核心的高级函数，其他多个键值对函数都是用它来实现的，如groupByKey，reduceByKey等等。

![image-20210111212007028](https://gitee.com/zisuu/picture/raw/master/img/20210111212007.png)

这是combineByKey的方法。可以看到主要有三个参数，后面还有分区等参数就不管了。主要来看前三个参数，分别是

createCombiner，mergeValue，mergeCombiners，参数类型是JFunction（接收一个参数，返回另一个类型的值），JFunction2（接收两个参数，返回另一个类型的值），JFunction2。

对一个PairRDD<K, V>做combineByKey操作的流程是这样：

1. createCombiner[V, C] 将当前的值V作为参数，然后对其进行一些操作或者类型转换等，相当于进行一次map操作，并返回map后的结果C。
2. mergeValue[C, V, C] 将createCombiner函数返回的结果C，再组合最初的PariRDD的V，将C和V作为输入参数，进行一些操作，并返回结果，类型也为C。
3. mergeCombiners[C, C] 将mergeValue产生的结果C，进行组合。这里主要是针对不同的分区，各自分区执行完上面两步后得到的C进行组合，最终得到结果。如果只有一个分区，那这个函数执行的结果，其实就是第二步的结果。

看例子，假如有多个学生，每个学生有多门功课的成绩，我们要计算每个学生的成绩平均分。

["zhangsan":10，"zhangsan":15]

注意，Key我们不用管，全程都用不到Key。我们需要做的就是对value的一系列转换。

通过第一步createCombiner将V转为C，做法是将10转为Tuple2，即第一次碰到zhangsan这个key时，变成{zhangsan:(10, 1)}，C就是Tuple2类型，目的是记录zhangsan这个key共出现了几次。

第二步mergeValue，输入是Tuple2和value，我们的做法就是将Tuple2的第一个参数加上value，然后将Tuple2的第二个参数加一。也就是又碰到zhangsan了，就用10+15，得到结果是{zhangsan:(25, 2)}.

第三步就是对第二步的结果进行合并，假设有另一个分区里，也有zhangsan的结果为{zhangsan:(30, 3)}.那么第三步就是将两个Tuple2分别相加。返回结果{zhangsan:(55, 5)}.

三步做完就可以collect了。

**combineByKey()允许用户返回值的类型与输入不一致。**

```
    val list: List[(String, Int)] = List(("a", 88), ("b", 95), ("a", 91), ("b", 93),
      ("a", 95), ("b", 98))
    val input= sc.makeRDD(list, 2)

    val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey( (_, 1),
      (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),
      (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
    )
```

> reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同
>
> FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同
>
> AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同
>
> CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同

**sortByKey & sortBy**

![image-20210111212746116](https://gitee.com/zisuu/picture/raw/master/img/20210111212746.png)



### 6.RDD序列化

### 8.RDD持久化

Spark主要是基于内存进行计算的，那么为什么我们对大量数据进行了很多的算子操作而内存不会爆掉？

在之前的文章中讲到Spark中有tranformation和action两类算子，tranformation算子具有lazy特性，只有action算子才会触发job的开始，从而去执行action算子之前定义的tranformation算子，从hdfs中读取数据等，计算完成之后，Spark会将内存中的数据清除，这样处理的好处是避免了OOM问题，但不好之处在于每次job都会从头执行一边，比如从hdfs上读取文件等，如果文件数据量很大，这个过程就会很耗性能。这个问题就涉及到本文要讲的RDD持久化特性，合理的使用RDD持久化对Spark的性能会有很大提升。

**1.1 首先看下如果不进行RDD持久化会有哪些影响？**

![img](https://gitee.com/zisuu/picture/raw/master/img/20210111214441.jpeg)

假设有一份文件，数据量很大，我们需要计算出这份文件的字数，功能实现其实很简单，利用下面的代码就可以实现

```scala
val lineRDD = sc.textFile("data/wc.txt")
val totalLength: Int = line.map(x => x.length).reduce(_+_)
```

假设统计了文件字数之后我们进行了某些操作，后面又需要对lineRDD进行操作，看一下如果不使用RDD持久化会带来哪些问题。

默认情况下，针对大量数据的action操作是很耗时的。Spark应用程序中，如果对某个RDD后面进行了多次transmation或者action操作，那么，可能每次都要重新计算一个RDD，那么就会反复消耗大量的时间，从而降低Spark的性能。第一次统计之后获取到了hdfs文件的字数，但是lineRDD会被丢弃掉，数据也会被新的数据填充，下次执行job的时候需要重新从hdfs上读取文件，不使用RDD持久化可能会导致程序异常的耗时。

**1.2 使用RDD持久化的好处**

![img](https://gitee.com/zisuu/picture/raw/master/img/20210111214609.jpeg)

对lineRDD执行持久化操作之后，虽然第一次统计操作执行完毕，但不会清除掉linesRDD中的数据，会将其缓存在内存中，或者磁盘上。第二次针对lineRDD执行操作时，此时就不会重新从hdfs中读取数据形成lineRDD，而是会直接从lineRDD所在的所有节点的内存缓存中，直接取出lineRDD的数据，对数据进行操作，那么使用了RDD持久化之后，只有在其第一次计算时才会进行计算，此后针对这个RDD所做的操作，就是针对其缓存了，就不需要多次计算同一个RDD，可以提升Spark程序的性能。

**2、RDD持久化原理**

Spark中非常重要的一个功能特性就是可以将RDD持久化在内存中，当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，直接使用内存缓存的partition。这样的话，对于针对一个RDD反复执行多个操作的场景，就只要针对RDD计算一次即可，后面直接使用该RDD，而不用反复计算该RDD。



**3、RDD持久化的使用场景**

RDD持久化虽然可以提高性能，但会消耗内存空间，一般用在如下场景：

- 1、第一次加载大量的数据到RDD中
- 2、频繁的动态更新RDD Cache数据，不适合使用Spark Cache、Spark lineage

**4、怎样使用RDD持久化**

要持久化一个RDD，只要调用其cache()或者persist()方法即可。在该RDD第一次被计算出来时，就会直接缓存在每个节点中，而且Spark的持久化机制是自动容错的，如果持久化的RDD的任何pratition丢失了，Spark会自动通过其源RDD，重新计算该parititon。



**5、通过cache()和persist()源码讲解RDD持久化策略级别**

通过cache()和persist()的源码我们可以看出两者的区别以及RDD持久化级别

```scala
def cache(): this.type = persist()
def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)

object StorageLevel {
  val NONE = new StorageLevel(false, false, false, false)
  val DISK_ONLY = new StorageLevel(true, false, false, false)
  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
  val MEMORY_ONLY = new StorageLevel(false, true, false, true)
  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)
```

cache()和persist()的区别在于，cahe()是persist()的一种简化方式，cache()的底层就是调用的persist()的无参版本，即persist(MEMORY_ONLY)，将数据持久化到内存中，如果需要从内存中清除缓存，调用unpersist()方法即可。Spark在进行shuffle操作的时候也会进行数据的持久化，比如写入磁盘，主要是为了避免节点失败时，重新计算整个过程。

RDD持久化是可以手动选择不同的策略的，默认是持久化到内存中的。还可以持久化到磁盘上，使用序列化的方式持久化，对持久化的数据进行复用，只要在调用persist()方法时传入对应的StorageLevel即可。

![img](https://pic2.zhimg.com/80/v2-0f0a744682e44c01093ed5260dde8f65_720w.jpg)

**6、RDD持久化策略选择**

优先使用MEMORY_ONLY，如果可以缓存所有数据的化，那么就使用这种级别，纯内存速度最快，而且没有序列化，不需要消耗CPU进行反序列化操作。

如果MEMORY_ONLY策略无法存储所有的数据的化，使用MEMORY_ONLY_SER，将数据进行序列化存储，节约空间，纯内存操作速度块，只是需要消耗cpu进行反序列化

如果需要进行快速的失败恢复，选择带后缀为2的策略，进行数据的备份，这样节点在失败时不需要重新计算

能不使用DISK相关的策略就不使用，有时从磁盘读取的速度还不如重新计算来的快。



### 9.RDD分区器

Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数。

**注意：**

**(1)只有Key-Value类型的RDD才有分区器的，非Key-Value类型的RDD分区器的值是None
(2)每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。**

 

**Hash分区**

HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除以分区的个数取余，如果余数小于0，则用余数+分区的个数（否则加0），最后返回的值就是这个key所属的分区ID。

查看hash分区原码如下：



```
 */
class HashPartitioner(partitions: Int) extends Partitioner {
  require(partitions >= 0, s"Number of partitions ($partitions) cannot be negative.")

  def numPartitions: Int = partitions

  def getPartition(key: Any): Int = key match {
    case null => 0//key为0则统统放入0号分区
    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)
//否则调用这个方法，根据key的hashcode和分区数，得到分区号
  }
def nonNegativeMod(x: Int, mod: Int): Int = {
  val rawMod = x % mod
  rawMod + (if (rawMod < 0) mod else 0)
}
```



 



**ranger分区**

HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。

RangePartitioner作用：将一定范围内的数映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大，但是分区内的元素是不能保证顺序。简单的说就是将一定范围内的数映射到某一个分区内。实现过程为：

第一步：先从整个RDD中抽取出样本数据，将样本数据排序，计算出每个分区的最大key值，形成一个Array[KEY]类型的数组变量rangeBounds；

第二步：判断key在rangeBounds中所处的范围，给出该key值在RDD中的分区id下标；该分区器要求RDD中的KEY类型必须是可以排序的

 



**自定义分区器**

要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面两个个方法。

（1）numPartitions: Int:返回创建出来的分区数。

（2）getPartition(key: Any): Int:返回给定键的分区编号(0到numPartitions-1)。

 

示例代码如下：将key对分区数求余数，得到分区号。



```
package partitioner

import org.apache.spark.Partitioner

class MyPartitioner(partitions:Int) extends Partitioner{

  override def numPartitions: Int = {
    return partitions;
  }

  override def getPartition(key: Any): Int = {

    val mykey : Int = key.asInstanceOf;
    //val i: Int = key.asInstanceOf[Int]
    return mykey%partitions ;
  }
}

def main(args: Array[String]): Unit = {

    val sc: SparkContext = new SparkContext(new SparkConf()
      .setMaster("local[*]").setAppName("spark"))

    val raw: RDD[(Int, String)] = sc.makeRDD(Array((1, "a"), (2, "b"), (3, "c"), (4, "d")))
    raw.saveAsTextFile("E:/idea/spark2/out/partitioner_before")

    val partitionedRDD: RDD[(Int, String)] = raw.partitionBy(new MyPartitioner(2))
    println(partitionedRDD.partitioner)
    partitionedRDD.saveAsTextFile("E:/idea/spark2/out/partitioner_after")
  }
```



查看分区后数据的分布：

0号文件：(2,b)(4,d)

1号文件：(1,a)(3,c)

使用自定义的 Partitioner 是很容易的:只要把它传给 partitionBy() 方法即可。Spark 中有许多依赖于数据混洗的方法，比如 join() 和 groupByKey()，它们也可以接收一个可选的 Partitioner 对象来**控制输出数据的分区方式。**

### 10.RDD文件读取与保存

## 三 累加器

来看一个简单的例子，需求是：统计单词的个数。

```
    val data: RDD[String] = sc.makeRDD(Seq("hadoop map reduce", "spark mllib"))
    // 方式1
    val count1: Int = data.flatMap(line => line.split(" ")).map(word => 1).reduce(_ + _)
    println(count1)
    // 方式2
    var acc = 0
    data.flatMap(line => line.split("")).map(word => {acc += 1; word})
    println(acc)
```

方式1使用spark提供的RDD算子实现需求，而方式2，我们在驱动程序中定义了一个变量`acc`，在map算子中每次加1来实现单词统计，最终的结果如下，
**输出**

```
5
0
```

可以发现，方式1正确实现了需求，而方式2却不行。这是因为，在驱动器程序中定义的变量，集群中运行的每个Task都会得到这些变量的一份新的副本，在Task中更新这些副本的值不会影响驱动器中的对应变量。
在处理分片时，如果想要实现更新共享变量的功能，就需要用到“累加器”。

### 系统累加器

Spark内置了三种类型的累加器，分别是

1. `LongAccumulator`用来累加整数型;
2. `DoubleAccumulator`用来累加浮点型;
3. `CollectionAccumulator`用来累加集合元素

```
val totalNum1: LongAccumulator = sc.longAccumulator("totalNum1")
val totalNum2: DoubleAccumulator = sc.doubleAccumulator("totalNum2")
val allWords: CollectionAccumulator[String] = sc.collectionAccumulator[String]("allWords")
data.foreach(
  line => {
    val words: Array[String] = line.split(" ")
    totalNum1.add(words.length)
    totalNum2.add(words.length)
    words.foreach(allWords.add(_))
  }
)
println(totalNum1.value)
println(totalNum2.value)
println(allWords.value)
5
5.0
[hadoop, map, reduce, spark, mllib]
```

累加器的`add(v)`方法将v添加进累加器（LongAccumulator和DoubleAccumulator为对值累加，CollectionAccumulator为将v添加进`_list: java.util.List[T]`），累加器的value用于获取累加器的值。

### 自定义累加器

有时候，Spark内置的累加器无法满足需求，可以自定义累加器。

1. 继承抽象类`AccumulatorV2[IN, OUT]`，重写相关方法；
2. 创建自定义Accumulator的实例，然后通过`SparkContext.register(acc: AccumulatorV2[_, _], name: String)`注册累加器。

`AccumulatorV2` 
自定义一个实现WordCount功能的累加器。

```
class MyAccumulator extends AccumulatorV2[String, util.Map[String, Int]] {
  private val _map: util.Map[String, Int] = Collections.synchronizedMap(new util.HashMap[String, Int]())

  override def isZero: Boolean = _map.isEmpty

  override def copyAndReset(): MyAccumulator = new MyAccumulator

  override def copy(): MyAccumulator = {
    val newAcc = new MyAccumulator
    _map.synchronized {
      newAcc._map.putAll(_map)
    }
    newAcc
  }

  override def reset(): Unit = _map.clear()

  override def add(v: String): Unit = {
    val i = _map.getOrDefault(v, 0)
    _map.put(v, i+1)
  }

  override def merge(other: AccumulatorV2[String, java.util.Map[String, Int]]): Unit = other match {
    case o: MyAccumulator => {
      val iter: util.Iterator[Map.Entry[String, Int]] = other.value.entrySet().iterator()
      while (iter.hasNext) {
        val entry: Map.Entry[String, Int] = iter.next()
        _map.put(entry.getKey, entry.getValue+_map.getOrDefault(entry.getKey, 0))
      }
    }
    case _ => throw new UnsupportedOperationException(
      s"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}")
  }

  override def value: java.util.Map[String, Int] = _map.synchronized {
    java.util.Collections.unmodifiableMap(new util.HashMap[String, Int](_map))
  }
}

 val myAcc: MyAccumulator = new MyAccumulator
    sc.register(myAcc, "myAcc")
    data.foreach(
      line => {
        val words = line.split(" ")
        words.foreach(myAcc.add(_))
      }
    )
 println(myAcc.value)
{reduce=1, hadoop=2, mllib=1, spark=1, map=1}
```

### 注意事项

1. 工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量；
2. 累加器的最终结果应该不受累加顺序的影响（CollectionAccumulator可以将结果集看做是一个可以有重复元素的无序Set）；
3. 如果累加器在spark的transform算子中调用add，可能会导致重复更新，最好将累加器的add操作放在 foreach() 这样的action算子中。

**错误用法**

```
val accum= sc.accumulator(0, "Error Accumulator")
val data = sc.parallelize(1 to 10)
//用accumulator统计偶数出现的次数，同时偶数返回0，奇数返回1
val newData = data.map{x => {
  if(x%2 == 0){
    accum += 1
      0
    }else 1
}}
//使用action操作触发执行
newData.count
//此时accum的值为5，是我们要的结果
accum.value

//继续操作，查看刚才变动的数据,foreach也是action操作
newData.foreach(println)
//上个步骤没有进行累计器操作，可是累加器此时的结果已经是10了
//这并不是我们想要的结果
accum.value

```

**正确用法:**

利用cache切除

```
//
val accum= sc.accumulator(0, "Error Accumulator")
val data = sc.parallelize(1 to 10)

//代码和上方相同
val newData = data.map{x => {...}}
//使用cache缓存数据，切断依赖。
newData.cache.count
//此时accum的值为5
accum.value

newData.foreach(println)
//此时的accum依旧是5
accum.value

```

**总结**

使用Accumulator时，为了保证准确性，只使用一次action操作。如果需要使用多次则使用cache或persist操作切断依赖。

## 四 广播变量

**一.原理**

广播变量：实际上就是Executor端用到了driver端的变量

如果在executor端你使用到了driver端的广播变量，如果不使用广播变量，在每个executor中有多少task就有多少变量副本。

注意:

1. 不能将RDD广播出去，RDD不存数据，可以将RDD的结果广播出去，rdd.collect()
2. 广播变量只能在Driver定义，在Executor端不能修改广播变量的值。

使用了广播变量，实际上就是为了减少executor端的备份，最终减少executor端的内存。

举个简单的例子：driver端有1000个变量，如果不使用广播变量的话，从driver端发给executor端有1000个备份，使用了广播变量，就会只有一个备份，从而可以减少executor端的内存。

```
val rdd1 = sc.makeRDD(List( ("a",1), ("b", 2), ("c", 3), ("d", 4) ),4)
val list = List( ("a",4), ("b", 5), ("c", 6), ("d", 7) )
// 声明广播变量
val broadcast: Broadcast[List[(String, Int)]] = sc.broadcast(list)

val resultRDD: RDD[(String, (Int, Int))] = rdd1.map { case (key, num) => {
var num2 = 0
// 使用广播变量
for ((k, v) <- broadcast.value) { if (k == key) {
num2 = v
}
}
(key, (num, num2))
}
}
```

