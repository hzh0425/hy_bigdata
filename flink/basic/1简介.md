• Flink 是什么

• 为什么要用 Flink

• 流处理的发展和演变

• Flink 的主要特点

• Flink vs Spark Streaming

## 一 flink是什么

Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数

据流进行状态计算。

![image-20210118090223599](https://gitee.com/zisuu/picture/raw/master/img/20210118090242.png)

## 二 为什么选择 Flink

• 流数据更真实地反映了我们的生活方式

• 传统的数据架构是基于有限数据集的

• 我们的目标

➢ 低延迟

➢ 高吞吐

➢ 结果的准确性和良好的容错性

### 哪些行业需要处理流数据

• 电商和市场营销

➢ 数据报表、广告投放、业务流程需要

• 物联网（IOT） 

➢ 传感器实时数据采集和显示、实时报警，交通运输业

• 电信业

➢ 基站流量调配

• 银行和金融业

➢ 实时结算和通知推送，实时检测异常行为

## 三 数据处理的演变过程

### 1.事务处理

- 将数据保存到关系型数据库(mysql),在通过关联连接查询进行统计分析

![image-20210118091055948](https://gitee.com/zisuu/picture/raw/master/img/20210118091056.png)

### 2.数仓ETL

- 将数据从业务数据库复制到数仓，再进行分析和查询

![image-20210118091159565](https://gitee.com/zisuu/picture/raw/master/img/20210118091159.png)

### 3.有状态的流(批)处理

- 第一批流式处理
- 通过一个本地内存中local state变量来保存数据,当有数据到大,就更新该状态
- 缺点:无法保证数据的顺序性

![image-20210118091318605](https://gitee.com/zisuu/picture/raw/master/img/20210118091318.png)

### 4.lambda架构:二代流处理

➢ 用两套系统，同时保证低延迟和结果准确

- 一套批处理,一套流处理
- 最后将批处理表和流处理表相结合,得到相关结果
- 缺点:要做两套系统,过于麻烦

![image-20210118091616772](https://gitee.com/zisuu/picture/raw/master/img/20210118091617.png)

### 5.flink:三代流处理

![image-20210118091812696](https://gitee.com/zisuu/picture/raw/master/img/20210118091812.png)

## 四 Flink主要特点

### 1.事件驱动

- 左边为传统事务型处理
- 右边为flink的架构
- 二者非常相似,都是来一个事件,处理一个事件

![image-20210118092429617](https://gitee.com/zisuu/picture/raw/master/img/20210118092429.png)

### 2.基于流的世界观

➢ 在 Flink 的世界观中，一切都是由流组成的，离线数据是有界的流；实时数据是一个没有界限的流：这就是所谓的有界流和无界流

- 对于无界流数据,来一个处理一个(和批处理最大的不同)
- 对于有界流(离线)数据,可以等到一个临界点,一起同时处理

![image-20210118092747969](https://gitee.com/zisuu/picture/raw/master/img/20210118092748.png)

### 3.分层API

➢ 越顶层越抽象，表达含义越简明，使用越方便

➢ 越底层越具体，表达能力越丰富，使用越灵活

- 最上层Table API,可以自行撰写SQL语句
- 中层Stream Api,可以进行流或批处理
- 最底层Api,不仅能获得事件,状态,还能获取事件,是最灵活,也是最复杂的Api

![image-20210118092926757](https://gitee.com/zisuu/picture/raw/master/img/20210118092926.png)

### 4.其他特点

• 支持事件时间（event-time）和处理时间（processing-time）语义

• 精确一次（exactly-once）的状态一致性保证

• 低延迟，每秒处理数百万个事件，毫秒级延迟

• 与众多常用存储系统的连接

• 高可用，动态扩展，实现7*24小时全天候运行

## 五 Spark VS Flink

- 本质上流处理和批处理的过程
- spark的本质还是攒一批后处理,会有延迟
- flink的本质是流处理,来一个处理一个,延迟极低

![image-20210118093603479](https://gitee.com/zisuu/picture/raw/master/img/20210118093603.png)

• **数据模型**

– spark 采用 RDD 模型，spark streaming 的 DStream 实际上也就是一组 组小批数据 RDD 的集合

– flink 基本数据模型是数据流，以及事件（Event）序列

**• 运行时架构**

– spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个

也即若有一个节点没处理完,后面的就不能做

– flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节

点进行处理

## 六 Flink快速上手

### 1.Maven工程

pom.xml

```
    <dependencies>

        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-java</artifactId>

            <version>1.10.1</version>

        </dependency>

        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-streaming-java_2.12</artifactId>

            <version>1.10.1</version>

        </dependency>

    </dependencies>
```



### 2.批处理

```java
public class PiWordCount {
    public static void main(String[] args) throws Exception {
        //执行环境
        ExecutionEnvironment env= ExecutionEnvironment.getExecutionEnvironment();
        // 从文件中读取数据
        String inputPath= "file/word.txt";
        DataSet<String> inputDataSet = env.readTextFile(inputPath);
        // 空格分词打散之后，对单词进行 groupby 分组，然后用 sum 进行聚合
        DataSet<Tuple2<String, Integer>> sum =
                inputDataSet.
                flatMap(new MyFlatMapper()).
                groupBy(0).
                sum(1);
        //打印
        sum.print();
    }
    public static class MyFlatMapper implements FlatMapFunction<String, Tuple2<String,Integer>>
    {

        @Override
        public void flatMap(String value, Collector<Tuple2<String, Integer>> collector) throws Exception {
            String[] words= value.split(" ");
            for(String word:words)
            {
                collector.collect(new Tuple2<String,Integer>(word,1));
            }
        }
    }
}

```

### 3.流处理

通过socket发送单词:

```java
public class LiuWordCount {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
        DataStreamSource<String> text = env.socketTextStream("localhost", 9000);
        DataStream<Tuple2<String, Integer>> wordCountDataStream=text.
                flatMap(new MyFlatMapper()).
                keyBy(0).
                sum(1);
        wordCountDataStream.print().setParallelism(1);
        env.execute();
    }
    public static class MyFlatMapper implements FlatMapFunction<String, Tuple2<String,Integer>>
    {

        @Override
        public void flatMap(String value, Collector<Tuple2<String, Integer>> collector) throws Exception {
            String[] words= value.split(" ");
            for(String word:words)
            {
                collector.collect(new Tuple2<String,Integer>(word,1));
            }
        }
    }
}

```

在cmd中输入:(建立socket连接)

```
nc -l - p 9000
```

