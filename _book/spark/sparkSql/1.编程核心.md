# 一 sparkSql概述

## 1.1 sparkSql是什么?

Spark SQL 是Spark 用于结构化数据(structured data)处理的 Spark 模块。

SparkSQL 的前身是 Shark，给熟悉RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具。

Hive 是早期唯一运行在Hadoop 上的SQL-on-Hadoop 工具。但是 MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL-on-Hadoop 的效率，大量的SQL-on-Hadoop 工具开始产生，其中表现较为突出的是：

- Drill

- Impala

- Shark

其中 Shark 是伯克利实验室 Spark 生态环境的组件之一，是基于Hive 所开发的工具，它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在 Spark 引擎上。

![img](https://gitee.com/zisuu/picture/raw/master/img/20210112093310.png)

但是，随着Spark 的发展，对于野心勃勃的Spark 团队来说，Shark 对于 Hive 的太多依赖（如采用 Hive 的语法解析器、查询优化器等等），制约了 Spark 的One Stack Rule Them All 的既定方针，制约了 Spark 各个组件的相互集成，所以提出了 SparkSQL 项目。SparkSQL 抛弃原有 Shark 的代码，汲取了 Shark 的一些优点，如内存列存储（In-Memory Columnar

Storage）、Hive 兼容性等，重新开发了SparkSQL 代码；由于摆脱了对Hive 的依赖性，SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便，真可谓“退一步，海阔天空”。

-  数据兼容方面 SparkSQL 不但兼容Hive，还可以从RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取RDBMS 数据以及 cassandra 等NOSQL 数据；

-  性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进Cost Model 对查询进行动态评估、获取最佳物理计划等等；

-  

组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。

![image-20210112093514342](https://gitee.com/zisuu/picture/raw/master/img/20210112093514.png)

其中 SparkSQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而Hive on Spark 是一个Hive 的发展计划，该计划将 Spark 作为Hive 的底层引擎之一，也就是说，Hive 将不再受限于一个引擎，可以采用 Map-Reduce、Tez、Spark 等引擎。

对于开发人员来讲，SparkSQL 可以简化RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。Spark SQL 为了简化RDD 的开发， 提高开发效率，提供了 2 个编程抽象，类似Spark Core 中的RDD

-  DataFrame

-  DataSet

## 1.2 sparkSql的特点

- 易整合

- 统一的数据访问:使用相同的方式连接不同的数据源

- 标准数据连接

  通过 JDBC 或者 ODBC 来连接

##  1.3 DataFrame是什么?

在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。

同时，与Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。

![image-20210112094240329](https://gitee.com/zisuu/picture/raw/master/img/20210112094240.png)

![img](https://gitee.com/zisuu/picture/raw/master/img/20210112094432.jpg)

上图直观地体现了DataFrame 和 RDD 的区别。

左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解Person 类的内部结构。而右侧的DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。

DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待

 DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。比如下面一个例子:

![img](https://gitee.com/zisuu/picture/raw/master/img/20210112094401.png)

为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个

DataFrame，将它们 join 之后又做了一次filter 操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为 join 是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter 下推到 join 下方，先对DataFrame 进行过滤，再 join 过滤后的较小的结果集，便可以有效缩短执行时间。而 Spark SQL 的查询优化器正是这样做的。简而言之， 逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。

## 1.4 DataSet是什么?

DataSet 是分布式数据集合。DataSet 是Spark 1.6 中添加的一个新抽象，是DataFrame

的一个扩展。它提供了RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及Spark

SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter

等等）。

-  DataSet 是DataFrame API 的一个扩展，是SparkSQL 最新的数据抽象

-  用户友好的 API 风格，既具有类型安全检查也具有DataFrame 的查询优化特性；

-  用样例类来对DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到

DataSet 中的字段名称；

-  DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。

-  DataFrame 是DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将

DataFrame 转换为DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序

# 二 sparkSql核心编程

本课件重点学习如何使用 Spark SQL 所提供的 DataFrame 和DataSet 模型进行编程.， 以及了解它们之间的关系和转换，关于具体的SQL 书写不是我们的重点。

Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext， Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。

在老的版本中，SparkSQL 提供两种 SQL 查询起始点：一个叫 SQLContext，用于 Spark

自己提供的SQL 查询；一个叫HiveContext，用于连接 Hive 的查询。

SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和HiveContext 的组合，所以在 SQLContex 和HiveContext 上可用的API 在 SparkSession 上同样是可以使用的。SparkSession 内部封装了 SparkContext，所以计算实际上是由 sparkContext 完成的。

idea导入:

```
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
```



## 1.DataFrame

### **1.1DataFrame的创建**

首先，让我们导入Spark需要的隐式函数，如.toDF()函数，并为示例创建数据。

```scala
val conf= new SparkConf().setMaster("local[*]").setAppName("count")
val spark= SparkSession.builder().config(conf).getOrCreate();\
val sc=spark.sparkContext
sc.setLogLevel("ERROR")
//引入隐式函数
import spark.implicits._
val columns = Seq("language", "users_count")
val data = Seq(("Java", "20000"), ("Python", "10000"), ("Scala", "30000"))
```

#### **1. from RDD**

首先，调用SparkContext中的[parallelize()](https://sparkbyexamples.com/tag/sparkcontext-parallelize/)函数从集合Seq创建RDD。对于下面的所有示例，都需要这个rdd对象。

```scala
val rdd = spark.SparkContext.parallelize(data)
```

**1. a) 使用toDF()函数**

一旦创建了一个RDD，可以使用toDF()来创建一个DataFrame。默认情况下，假如数据集每一行有两列，创建的DF时候的列名就是"_1"和"_2"。

```scala
val dfFromRDD1 = rdd.toDF()
dfFromRDD1.printSchema()

root
|-- _1: string (nullable = true)
|-- _2: string (nullable = true)
```

toDF()具有另一个签名，该签名自定义列名称参数，如下所示：

```scala
val dfFromRDD1 = rdd.toDF("language", "users_count")
dfFromRDD1.printSchema()

root
|-- language: string (nullable = true)
|-- users: string (nullable = true)
```

默认情况下，这些列的数据类型是通过推断列的数据类型来判断的。我们可以通过提供模式来更改此行为，我们可以在其中为每个字段/列指定列名，数据类型和可为空。

**1.b) 使用SparkSession的creatDataFrame()函数**

使用SparkSession中的createDataFrame()是另一种创建方法，它以rdd对象作为参数。使用toDF()来指定列的名称。

```scala
dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)
```

**1.c)对行类型使用createDataFrame()**

createDataFrame()有另一个签名，它将列名的RDD[Row]类型和模式作为参数。首先，我们需要将rdd对象从RDD[T]转换为RDD[Row]类型。

```scala
val schema = StructType(columns.map(fieldName => StructField(fieldName, StringType, nullable = true)))
val rowRDD = rdd.map(attributes => Row(attributes._1, attributs._2))
val dfFromRDD3 = spark.createDataFrame(rowRdd.schema)
```

#### **2. from List和Seq**

在本节中，我们将看到从集合Seq[T]或List[T]创建Spark DataFrame的几种方法。这些示例与我们上面的ＲＤＤ部分看到的类型，但是我们使用的是数据对象而不是ＲＤＤ对象。

**2.a) List或者Seq使用toDF()**

```scala
val dfFromData1 = data.toDF()
```

**2.b) 使用SparkSession的createDataFrame()方法**

```scala
var dfFromData2 = spark.createDataFrame(data).toDF(columns:_*)
```

**2.c) 使用Row type的ｃreateDataFrame()方法**

```scala
import scala.collection.JavaConversions._
val rowData = data.map(attributes => Row(attributes._1, attributes._2))
var dfFromData3 = spark.createDataFrame(rowData, schema)
```

#### 3.from file

**3a. 从CSV文件创建Spark DataFrame**

```scala
val df2 = spark.read.csv("/src/resources/file.csv")
```

**3b. 从text文件创建**

```scala
val df2 = spark.read.text("/src/resources/file.txt")
```

**3c. 从JSON文件创建**

```scala
val df2 = spark.read.json("/src/resources/file.json")
```



**4. from Hive**

```scala
val hiveContext = new org.apache.spark.sql.hive.HiveContext(spark.sparkContext)
val hiveDF = hiveContext.sql("select * from emp")
```

#### **4. from RDBMS**

**8.a) Mysql table**

确保在pom.xml文件或类路径中的MySQL jars中都具有Mysql库作为依赖项

```scala
val df_mysql = spark.read.format("jdbc")
    .option("url", "jdbc:mysql://localhost:port/db")
    .option("driver", "com.mysql.cj.jdbc.Driver")
    .option("dbtable", "tablename")
    .option("user", "user")
    .option("password", "password")
    .load()
```

**8.b) DB2**

确保在pom.xml文件或类路径中的DB2 jar中将DB2库作为依赖项。

```scala
val df_db2 = spark.read.format(“jdbc”)
   .option(“url”, “jdbc:db2://localhost:50000/dbname”)
   .option(“driver”, “com.ibm.db2.jcc.DB2Driver”)
   .option(“dbtable”, “tablename”) 
   .option(“user”, “user”) 
   .option(“password”, “password”) 
   .load()
```

### 1.2 Data Frame sql语法

SQL 语法风格是指我们查询数据的时候使用 SQL 语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助

SparkSession的临时表分为两种

- 全局临时表：作用于某个Spark应用程序的所有SparkSession会话
- 局部临时表：作用于某个特定的SparkSession会话

如果同一个应用中不同的session需要重用一个临时表，那么不妨将该临时表注册为全局临时表，可以避免多余的IO，提高系统的执行效率，但是如果只是在某个session中使用，只需要注册局部临时表，可以避免不必要的内存占用

```
package sparksql

import org.apache.spark.sql.SparkSession

object SparkSqltest1 {

  def main(args: Array[String]): Unit = {
    //创建sparksession
    val   sparkSession=SparkSession.builder().appName("test1").master("local[*]")getOrCreate()
    import  sparkSession.implicits._
    //读取文件形成dataframe
    val   df=sparkSession.read.json("data.json")
    val   temp=df.createTempView("temp_person")   //局部变量表
    sparkSession.sql("select  *  from  temp_person").show()
    val   glob=df.createGlobalTempView("glob_person")   //全局变量表
    sparkSession.sql("select  *  from  glob_person").show()
    sparkSession.newSession().sql("select  *  from  glob_person").show()   //使用newSession()创建一个新的session
    sparkSession.stop()
  }
}
```

运行的结果为：

```
+---+--------+
|age|    name|
+---+--------+
|  1|zhangsan|
|  2|    lisi|
+---+--------+
+---+--------+
|age|    name|
+---+--------+
|  1|zhangsan|
|  2|    lisi|
+---+--------++
|age|    name|
+---+--------+
|  1|zhangsan|
|  2|    lisi|
+---+--------+
```

### 1.3 DSL语法

DataFrame 提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了

```
    //数据
    val data = spark.read.json("file/person.json")
    //查看username列数据
    data.select("name").show()
    //查看username以及age+1 的数据
    //注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名
    data.select($"name",$"age"+1).show()
    //查看age>30
    data.filter($"age">30).show()
    //按age分组
    data.groupBy("age").count().show();
```

还有其他json,map等操作

## 2 DataSet

DataSet 是具有强类型的数据集合，需要提供对应的类型信息。

```
  {
  //数据
    val df = spark.read.json("file/person.json")
    val ds= df.as[Person]
    ds.show()
  }
  case class Person(name:String,age:Long)
```

## 3.rdd,dataFrame,dataSet三者的关系

在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看：

-  Spark1.0 => RDD

-  Spark1.3 => DataFrame

-  Spark1.6 => Dataset

如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的 Spark 版本中，DataSet 有可能会逐步取代RDD和 DataFrame 成为唯一的API 接口。

### 1.三者区别

**1) RDD**

-  RDD 一般和 spark mllib 同时使用

-  RDD 不支持 sparksql 操作

**2) DataFrame**

-  与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值

-  DataFrame 与DataSet 一般不与 spark mllib 同时使用

-  DataFrame 与DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作

-  DataFrame 与DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)

**3) DataSet**

-  Dataset 和DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。

DataFrame 其实就是DataSet 的一个特例	type DataFrame = Dataset[Row]

-  DataFrame 也可以叫Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息

### 2.相互转化

```
    //引入隐式函数
    import spark.implicits._
    //数据
    val df=spark.rdd.toDF()
    val ds=spark.rdd.toDS().as[Person]
    df.show()
    ds.show()
    
    //ds,df->rdd
    ds.rdd
    df.rdd
```

## 4.自定义函数

> **UDF：**
>
> UDF（User-defined functions）用户自定义函数，简单说就是输入一行输出一行的自定义算子。
> 是大多数 SQL 环境的关键特性，用于扩展系统的内置功能。**（一对一）**
>
> **UDAF**
>
> UDAF（User Defined Aggregate Function），即用户定义的聚合函数，聚合函数和普通函数的区别是什么呢，普通函数是接受一行输入产生一个输出，聚合函数是接受一组（一般是多行）输入然后产生一个输出，即将一组的值想办法聚合一下。**（多对一）**
>
> UDAF可以跟group by一起使用，也可以不跟group by一起使用，这个其实比较好理解，联想到mysql中的max、min等函数，可以:
> select max(foo) from foobar group by bar;
> 表示根据bar字段分组，然后求每个分组的最大值，这时候的分组有很多个，使用这个函数对每个分组进行处理，也可以：
> select max(foo) from foobar;
> 这种情况可以将整张表看做是一个分组，然后在这个分组（实际上就是一整张表）中求最大值。所以聚合函数实际上是对分组做处理，而不关心分组中记录的具体数量。
>
> **UDTF**
>
> UDTF(User-Defined Table-Generating Functions),用户自定义生成函数。它就是输入一行输出多行的自定义算子，可输出多行多列，又被称为 “表生成函数”。**（一对多）**

### 1.UDF

#### 1、创建测试用DataFrame

```scala
// 构造测试数据，有两个字段、名字和年龄
val userData = Array(("Leo", 16), ("Marry", 21), ("Jack", 14), ("Tom", 18))

//创建测试df
val userDF = spark.createDataFrame(userData).toDF("name", "age")
userDF.show123456
+-----+---+
| name|age|
+-----+---+
|  Leo| 16|
|Marry| 21|
| Jack| 14|
|  Tom| 18|
+-----+---+12345678
// 注册一张user表
userDF.createOrReplaceTempView("user")
```

#### 2、Spark Sql用法

**2.1 通过匿名函数注册UDF**

下面的UDF的功能是计算某列的长度，该列的类型为String

**2.1.1 注册**

```scala
spark.udf.register("strLen", (str: String) => str.length())1
```

**2.2.2 使用**

```scala
spark.sql("select name,strLen(name) as name_len from user").show1
+-----+--------+
| name|name_len|
+-----+--------+
|  Leo|       3|
|Marry|       5|
| Jack|       4|
|  Tom|       3|
+-----+--------+12345678
```

**2.2 通过实名函数注册UDF**

实名函数的注册有点不同，要在后面加 _(注意前面有个空格)
定义一个实名函数

```scala
/**
 * 根据年龄大小返回是否成年 成年：true,未成年：false
*/
def isAdult(age: Int) = {
  if (age < 18) {
    false
  } else {
    true
  }

}
```

```scala
spark.udf.register("isAdult", isAdult _)
```

至于使用都是一样的

**2.3 关于spark.udf和sqlContext.udf**

在Spark2.x里，两者实际最终都是调用的spark.udf
sqlContext.udf源码

```scala
def udf: UDFRegistration = sparkSession.udf
```

可以看到调用的是sparkSession的udf，即spark.udf

#### 3、DataFrame用法

DataFrame的udf方法虽然和Spark Sql的名字一样，但是属于不同的类，它在org.apache.spark.sql.functions里，下面是它的用法

**3.1注册**

```scala
import org.apache.spark.sql.functions._
//注册自定义函数（通过匿名函数）
val strLen = udf((str: String) => str.length())
//注册自定义函数（通过实名函数）
val udf_isAdult = udf(isAdult _)12345
```

**3.2 使用**

可通过withColumn和select使用,下面的代码已经实现了给user表添加两列的功能
\* 通过看源码，下面的withColumn和select方法Spark2.0.0之后才有的，关于spark1.xDataFrame怎么使用注册好的UDF没有研究

```scala
//通过withColumn添加列
userDF.withColumn("name_len", strLen(col("name"))).withColumn("isAdult", udf_isAdult(col("age"))).show
//通过select添加列
userDF.select(col("*"), strLen(col("name")) as "name_len", udf_isAdult(col("age")) as "isAdult").show1234
```

结果均为

```vim
+-----+---+--------+-------+
| name|age|name_len|isAdult|
+-----+---+--------+-------+
|  Leo| 16|       3|  false|
|Marry| 21|       5|   true|
| Jack| 14|       4|  false|
|  Tom| 18|       3|   true|
+-----+---+--------+-------+12345678
```

**3.3 withColumn和select的区别**

可通过withColumn的源码看出withColumn的功能是实现增加一列，或者替换一个已存在的列，他会先判断DataFrame里有没有这个列名，如果有的话就会替换掉原来的列，没有的话就用调用select方法增加一列，所以如果我们的需求是增加一列的话，两者实现的功能一样，且最终都是调用select方法，但是withColumn会提前做一些判断处理，所以withColumn的性能不如select好。
\* 注：select方法和sql 里的select一样，如果新增的列名在表里已经存在，那么结果里允许出现两列列名相同但数据不一样，大家可以自己试一下。

```scala
/**
 * Returns a new Dataset by adding a column or replacing the existing column that has
 * the same name.
 *
 * @group untypedrel
 * @since 2.0.0
*/
def withColumn(colName: String, col: Column): DataFrame = {
  val resolver = sparkSession.sessionState.analyzer.resolver
  val output = queryExecution.analyzed.output
  val shouldReplace = output.exists(f => resolver(f.name, colName))
  if (shouldReplace) {
    val columns = output.map { field =>
      if (resolver(field.name, colName)) {
        col.as(colName)
      } else {
        Column(field)
      }
    }
    select(columns : _*)
  } else {
    select(Column("*"), col.as(colName))
  }
}
```



### 2.UDAF

强类型的Dataset 和弱类型的 DataFrame 都提供了相关的聚合函数， 如 count()，

countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承 UserDefinedAggregateFunction 来实现用户自定义弱类型聚合函数。从Spark3.0 版本后，UserDefinedAggregateFunction 已经不推荐使用了。可以统一采用强类型聚合函数

#### 2.1 继承UserDefinedAggregateFunction

使用UserDefinedAggregateFunction的套路：

1. 自定义类继承UserDefinedAggregateFunction，对每个阶段方法做实现

2. 在spark中注册UDAF，为其绑定一个名字

3. 然后就可以在sql语句中使用上面绑定的名字调用

下面写一个计算平均值的UDAF例子，首先定义一个类继承UserDefinedAggregateFunction：

```
package cc11001100.spark.sql.udaf
 
import org.apache.spark.sql.Row
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._
 
object AverageUserDefinedAggregateFunction extends UserDefinedAggregateFunction {
 
  // 聚合函数的输入数据结构
  override def inputSchema: StructType = StructType(StructField("input", LongType) :: Nil)
 
  // 缓存区数据结构
  override def bufferSchema: StructType = StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)
 
  // 聚合函数返回值数据结构
  override def dataType: DataType = DoubleType
 
  // 聚合函数是否是幂等的，即相同输入是否总是能得到相同输出
  override def deterministic: Boolean = true
 
  // 初始化缓冲区
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0L
    buffer(1) = 0L
  }
 
  // 给聚合函数传入一条新数据进行处理
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (input.isNullAt(0)) return
    buffer(0) = buffer.getLong(0) + input.getLong(0)
    buffer(1) = buffer.getLong(1) + 1
  }
 
  // 合并聚合函数缓冲区
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }
 
  // 计算最终结果
  override def evaluate(buffer: Row): Any = buffer.getLong(0).toDouble / buffer.getLong(1)
 
}
```

然后注册并使用它：

```
package cc11001100.spark.sql.udaf
 
import org.apache.spark.sql.SparkSession
 
object SparkSqlUDAFDemo_001 {
 
  def main(args: Array[String]): Unit = {
 
    val spark = SparkSession.builder().master("local[*]").appName("SparkStudy").getOrCreate()
    spark.read.json("data/user").createOrReplaceTempView("v_user")
    spark.udf.register("u_avg", AverageUserDefinedAggregateFunction)
    // 将整张表看做是一个分组对求所有人的平均年龄
    spark.sql("select count(1) as count, u_avg(age) as avg_age from v_user").show()
    // 按照性别分组求平均年龄
    spark.sql("select sex, count(1) as count, u_avg(age) as avg_age from v_user group by sex").show()
 
  }
 
}
```

使用到的数据集：

```
{"id": 1001, "name": "foo", "sex": "man", "age": 20}
{"id": 1002, "name": "bar", "sex": "man", "age": 24}
{"id": 1003, "name": "baz", "sex": "man", "age": 18}
{"id": 1004, "name": "foo1", "sex": "woman", "age": 17}
{"id": 1005, "name": "bar2", "sex": "woman", "age": 19}
{"id": 1006, "name": "baz3", "sex": "woman", "age": 20}
```

运行结果：

![image](https://gitee.com/zisuu/picture/raw/master/img/20210112105144.png)

![image](https://images2018.cnblogs.com/blog/784924/201808/784924-20180814001734512-1493646988.png)

 

#### 2.2 继承Aggregator

还有另一种方式就是继承Aggregator这个类，优点是可以带类型：

```
package cc11001100.spark.sql.udaf
 
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.{Encoder, Encoders}
 
/**
  * 计算平均值
  *
  */
object AverageAggregator extends Aggregator[User, Average, Double] {
 
  // 初始化buffer
  override def zero: Average = Average(0L, 0L)
 
  // 处理一条新的记录
  override def reduce(b: Average, a: User): Average = {
    b.sum += a.age
    b.count += 1L
    b
  }
 
  // 合并聚合buffer
  override def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
 
  // 减少中间数据传输
  override def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
 
 //DataSet 默认额编解码器，用于序列化，固定写法
//自定义类型就是 product 自带类型根据类型选择
  override def bufferEncoder: Encoder[Average] = Encoders.product
 
  // 最终输出结果的类型
  override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
 
}
 
/**
  * 计算平均值过程中使用的Buffer
  *
  * @param sum
  * @param count
  */
case class Average(var sum: Long, var count: Long) {
}
 
case class User(id: Long, name: String, sex: String, age: Long) {
}
```

调用：

```
   val ds= spark.read.json("file/person/json").as[User]
    ds.createTempView("user")
    val udaf= new myUDAF
    spark.udf.register("avgAge",functions.udaf(udaf))
    spark.sql("select avgAge(age) from user").show()

```

运行结果：

![image](https://images2018.cnblogs.com/blog/784924/201808/784924-20180814000356253-1035361926.png) 





