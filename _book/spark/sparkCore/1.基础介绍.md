## 一 Spark运行架构

### Spark软件栈

![这里写图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110220507.png)

- Spark Core:

包含Spark的基本功能，包含任务调度，内存管理，容错机制等，内部定义了RDDs(弹性分布式数据集)，提供了很多APIs来创建和操作这些RDDs。为其他组件提供底层的服务。

- Spark SQL:

Spark处理结构化数据的库，就像Hive SQL,Mysql一样，企业中用来做报表统计。

- Spark Streaming:

实时数据流处理组件，类似Storm。Spark Streaming提供了API来操作实时流数据。企业中用来从Kafka接收数据做实时统计。

- MLlib:

一个包含通用机器学习功能的包，Machine learning lib包含分类，聚类，回归等，还包括模型评估和数据导入。MLlib提供的上面这些方法，都支持集群上的横向扩展。

- Graphx:

处理图的库（例如，社交网络图），并进行图的并行计算。像Spark Streaming,Spark SQL一样，它也继承了RDD API。它提供了各种图的操作，和常用的图算法，例如PangeRank算法。

Spark提供了全方位的软件栈，只要掌握Spark一门编程语言就可以编写不同应用场景的应用程序（批处理，流计算，图计算等）。Spark主要用来代替Hadoop的MapReduce部分。

Hadoop MapReduce缺点：

1. 表达能力有限
2. 磁盘IO开销大，任务之间的衔接涉及IO开销
3. 延迟高，Map任务要全部结束，reduce任务才能开始。

Spark借鉴Hadoop MapReduce优点的同时，解决了MapReuce所面临的问题，有如下优点：

1. Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供多种数据集操作类型，编程模型比Hadoop MapReduce更灵活。
2. Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高
3. Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制。

### spark核心概念

#### Executor

Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。

![image-20210110222155182](https://gitee.com/zisuu/picture/raw/master/img/20210110222155.png)

#### 并行度

在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。

设置

并行度控制任务的数量，影响shuffle操作后数据被切分成的块数。调整并行度让任务的数量和每个任务处理的数据与机器的处理能力达到最优

查看CPU使用情况和内存占用情况，当任务和数据不是平均分布在各节点，而是集中在个别节点时，可以增大并行度使任务和数据更均匀的分布在各个节点。增加任务的并行度，充分利用集群机器的计算能力，一般并行度设置为集群CPU总和的2-3倍

- **操作步骤**

并行度可以通过如下三种方式来设置，可以根据实际的内存、CPU、数据以及应用程序逻辑的情况调整并行度参数

● 在会产生shuffle的操作函数内设置并行度参数，优先级最高

> testRDD.groupByKey(24)

● 在代码中配置“spark.default.parallelism”设置并行度，优先级次之

> val conf = new SparkConf()
>
> conf.set("spark.default.parallelism", 24)

● 在 “$SPARK_HOME/conf/spark-defaults.conf” 文件中配置“spark.default.parallelism”的值，优先级最低

> spark.default.parallelism 24

#### DAG有向无环图

![image-20210110222436746](https://gitee.com/zisuu/picture/raw/master/img/20210110222436.png)

> **关于DAG的介绍**
>
> DAG是有向无环图（Directed Acyclic Graph）的简称。在大数据处理中，DAG计算常常指的是将计算任务在内部分解成为若干个子任务，将这些子任务之间的逻辑关系或顺序构建成DAG（有向无环图）结构。
> DAG在分布式计算中是非常常见的一种结构，在各个细分领域都可以看见它，比如Dryad,Flumejava和Tez，都是明确构建DAG计算模型的典型，再如流式计算的Storm等系统或机器学习框架Spark等，其计算任务大多也是DAG形式出现的，除此外还有很多场景都能见到。
>
> DAG计算的三层结构：
> 最上层是应用表达层，即是通过一定手段将计算任务分解成由若干子任务形成的DAG结构，其核心是表达的便捷性，主要是方便应用开发者快速描述或构建应用。
> 中间层是DAG执行引擎层，主要目的是将上层以特殊方式表达的DAG计算任务通过转换和映射，将其部署到下层的物理机集群中运行，这层是DAG计算的核心部件，计算任务的调度，底层硬件的容错，数据与管理信息的传递，整个系统的管理与正常运转等都需要由这层来完成。
> 最下层是物理机集群，即由大量物理机器搭建的分布式计算环境，这是计算任务最终执行的场所。

大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop 所承载的 MapReduce,它将计算分为两个阶段，分别为 Map 阶段 和 Reduce 阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。

这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行

### Spark运行架构及流程

![这里写图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110220509.png)

基本概念：

- Application：用户编写的Spark应用程序。
- Driver：Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭。
- Executor：Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程
- RDD：弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。
- DAG：有向无环图，反映RDD之间的依赖关系。
- Task：运行在Executor上的工作单元。
- Job：一个Job包含多个RDD及作用于相应RDD上的各种操作。
- Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表一组关联的，相互之间没有Shuffle依赖关系的任务组成的任务集。
- Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型
  1) Standalon : spark原生的资源管理，由Master负责资源的分配
  2) Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架
  3) Hadoop Yarn: 主要是指Yarn中的ResourceManager

一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成。

当执行一个Application时，Driver会向集群管理器申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其它数据库中。

与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点：

1. 利用多线程来执行具体的任务减少任务的启动开销；
2. Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销；

Spark运行基本流程：

![这里写图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110220512.png)

1. 为应用构建起基本的运行环境，即由Driver创建一个SparkContext进行资源的申请、任务的分配和监控
2. 资源管理器为Executor分配资源，并启动Executor进程
3. SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理。
   Executor向SparkContext申请Task，TaskScheduler将Task发放给Executor运行并提供应用程序代码。
4. Task在Executor上运行把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源。

Spark运行架构特点：

1. 每个Application都有自己专属的Executor进程，并且该进程在Application运行期间一直驻留。Executor进程以多线程的方式运行Task。
2. Spark运行过程与资源管理器无关，只要能够获取Executor进程并保存通信即可。
3. Task采用数据本地性和推测执行等优化机制。



## 三 sparkRDD

一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群的不同节点上进行并行计算。

RDD提供了一种高端受限的共享内存模型，即RDD是只读的记录分区的集合，**不能直接修改**，只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作（如map，join和group by）而创建得到新的RDD。

### RDD执行过程

1. RDD读入外部数据源进行创建
2. RDD经过一系列的转换（Transformation）操作，没一次都会产生不同的RDD供下一个转换操作使用
3. 最后一个RDD经过“动作”操作进行转换并输出到外部数据源

### RDD的特点

优点：**惰性调用**、管道化、避免同步等待，不需要保存中间结果。这和Java8中Stream的概念极其类似。
![这里写图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110221132.png)
RDD特性

1. 高效的容错性，根据DAG图恢复分区，数据复制或者记录日志
   RDD血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作
2. **中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销**
3. 存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化

### 窄依赖和宽依赖

![image-20210110221913980](https://gitee.com/zisuu/picture/raw/master/img/20210110221914.png)

基于此图，分析下这里为什么前面的流程都是窄依赖，而后面的却是宽依赖：

我们仔细看看，map和filter算子中，对于父RDD来说，一个分区内的数据，有且仅有一个子RDD的分区来消费该数据。

同样，UNION算子也是同样的：

所以，我们判断窄依赖的依据就是：父类分区内的数据，会被子类RDD中的指定的唯一一个分区所消费：

这是很重要的：

面试的时候，面试官问到了一个问题，如果父类RDD有很多的分区，而子类RDD只有一个分区，我们可以使用repartition或者coalesce算子来实现该效果，请问，这种实现是宽依赖？还是窄依赖？

如果从网上流传的一种观点：子RDD一个partition内的数据依赖于父类RDD的所有分区，则为宽依赖，这种判断明显是错误的：

别笑，网上的确有这种说法，我差点栽了跟头，这种解释实质上是错误的，因为如果我们的reduceTask只有一个的时候，只有一个分区，这个分区内的数据，肯定依赖于所有的父类RDD：

毫无疑问，这是个窄依赖：

相对之下，什么是宽依赖呢？

宽依赖，指的是父类一个分区内的数据，会被子RDD内的多个分区消费，需要自行判断分区，来实现数据发送的效果：

总结一下：

窄依赖：父RDD中，每个分区内的数据，都只会被子RDD中特定的分区所消费，为窄依赖：

宽依赖：父RDD中，分区内的数据，会被子RDD内多个分区消费，则为宽依赖：

这里，还存在一个可能被挑刺的地方，比如说父类每个分区内都只有一个数据，毫无疑问，这些数据都会被唯一地指定到子类的某个分区内，这是窄依赖？还是宽依赖？

这时候，可以从另外一个角度来看问题：

每个分区内的数据，是否能够指定自己在子类RDD中的分区？

如果不能，那就是宽依赖：如果父RDD和子RDD分区数目一致，那基本就是窄依赖了：

总之，还是要把握住根本之处，就是父RDD中分区内的数据，是否在子类RDD中也完全处于一个分区，如果是，窄依赖，如果不是，宽依赖。

### Stage的划分

Spark通过分析各个RDD的依赖关系生成了DAG,在通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage。具体划分方法如下：

- 在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到Stage中；
- 将窄依赖尽量划分在同一个Stage中，可以实现流水线计算
  ![这里写图片描述](https://gitee.com/zisuu/picture/raw/master/img/20210110221847.png)