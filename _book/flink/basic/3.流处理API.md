# Flink流处理API

## 一 Transform

### 1.map/flatMap

map一对一变化,flatMap一变多

```
DataStream<Integer> mapStram = dataStream.map(new MapFunction<String, Integer>() {
    public Integer map(String value) throws Exception {
    return value.length();
    }
});
```

```
DataStream<String> flatMapStream = dataStream.flatMap(new FlatMapFunction<String, String>() {
        public void flatMap(String value, Collector<String> out) throws Exception { 		String[] fields = value.split(",");
    for( String field: fields )
    out.collect(field);
}
});
```

### 2.filter

```
DataStream<Interger> filterStream = dataStream.filter(new FilterFunction<String>()
{
    public boolean filter(String value) throws Exception {
    return value == 1;
    }
});
```

### 3.KeyBy

![image-20210119001637718](https://gitee.com/zisuu/picture/raw/master/img/20210119001637.png)

***\*DataStream\**** ***\*→\**** ***\*KeyedStream\****： 逻辑地将一个流拆分成不相交的分区，每个分区包含具有相同 key 的元素， 在内部以 hash 的形式实现的。

```java
        DataStream<Tuple2<String, Integer>> wordCountDataStream=text.
                flatMap(new MyFlatMapper()).
                keyBy(0).
                sum(1);
```

### 4.滚动聚合算子

这些算子可以针对 KeyedStream 的每一个支流做聚合。

sum()

min()

max()

minBy()

maxBy()

```java
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        // 从文件中读取数据
        String inputPath= "file/read.txt";
        DataStream<String> inputDataSet = env.readTextFile(inputPath);
        DataStream<sensorRead> mapStream = inputDataSet.map(line -> {
            String[] fields = line.split(" ");
            return new sensorRead(fields[0], new Integer(fields[1]));
        });
        //分组
        KeyedStream<sensorRead, Tuple> keyStream = mapStream.keyBy("id");
        //求最大值
        DataStream<sensorRead> value = keyStream.maxBy("value");
        value.print();
        env.execute();
    }
        
input:
aaa 1
aaa 5
bbb 6
bbb 11
ccc 2
ccc 90

output:
sensorRead{id='aaa', value=1}
sensorRead{id='aaa', value=5}
sensorRead{id='bbb', value=6}
sensorRead{id='bbb', value=11}
sensorRead{id='ccc', value=2}
sensorRead{id='ccc', value=90}

所谓滚动聚合，指的是来一条，更新一条！！！！
```

### 5.reduce

***\*KeyedStream\**** ***\*→\**** ***\*DataStream\****： 一个分组数据流的聚合操作， 合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。

```java
        // 从文件中读取数据
        String inputPath= "file/read.txt";
        DataStream<String> inputDataSet = env.readTextFile(inputPath);
        DataStream<sensorRead> mapStream = inputDataSet.map(line -> {
            String[] fields = line.split(" ");
            return new sensorRead(fields[0], new Integer(fields[1]));
        });
        //分组
        KeyedStream<sensorRead, Tuple> keyStream = mapStream.keyBy("id");
        //求和
        SingleOutputStreamOperator<sensorRead> value = keyStream.reduce(new ReduceFunction<sensorRead>() {
            @Override
            public sensorRead reduce(sensorRead v1, sensorRead v2) throws Exception {
                return new sensorRead(v1.getId(), v1.value + v2.value);
            }
        });
        value.print();
        env.execute();
        
        
sensorRead{id='aaa', value=1}
sensorRead{id='aaa', value=6}
sensorRead{id='bbb', value=6}
sensorRead{id='bbb', value=17}
sensorRead{id='ccc', value=2}
sensorRead{id='ccc', value=92}
```

### 6.split & select

***\*DataStream\****  ***\*→\**** ***\*SplitStream\****：根据某些特征把一个 DataStream 拆分成两个或者多个 DataStream。

![image-20210120203222916](https://gitee.com/zisuu/picture/raw/master/img/20210120203252.png)

 

***\*SplitStream\*******\*→\*******\*DataStream\****： 从一个 SplitStream 中获取一个或者多个DataStream。

![image-20210120203248259](https://gitee.com/zisuu/picture/raw/master/img/20210120203248.png)

```
       
        SplitStream<sensorRead> splitStream = mapStream.split(new OutputSelector<sensorRead>() {
            @Override
            public Iterable<String> select(sensorRead sensorRead) {
                return sensorRead.value > 10 ? Collections.singletonList("high") : Collections.singletonList("low");
            }

            ;
        });
        DataStream<sensorRead> high = splitStream.select("high");
        DataStream<sensorRead> low = splitStream.select("low");
        high.print("high");
        low.print("low");
        env.execute();
```

### 7.connect & coMap

connect:

![image-20210120204855115](https://gitee.com/zisuu/picture/raw/master/img/20210120204855.png)

***\*DataStream,DataStream\**** ***\*→\**** ***\*ConnectedStreams\****：连接两个保持他们类型的数据流，两个数据流被 Connect 之后，只是被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立

![image-20210120204905479](https://gitee.com/zisuu/picture/raw/master/img/20210120204905.png)

***\*ConnectedStreams → DataStream\**** ：作用于 ConnectedStreams 上， 功能与 map 和 flatMap 一样，对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。

```
        //将high转化为元祖，与low合并，最后一起转化为tuple
        DataStream<Tuple2> warning = high.map(sensor -> new Tuple2(sensor.getId(), sensor.getValue()));
        ConnectedStreams<Tuple2, sensorRead> connectStream = warning.connect(low);
        DataStream<Object> coStream = connectStream.map(new CoMapFunction<Tuple2, sensorRead, Object>() {
            @Override
            public Object map1(Tuple2 tuple2) throws Exception {
                return tuple2;
            }

            @Override
            public Object map2(sensorRead sensorRead) throws Exception {
                return new Tuple2<>(sensorRead.getId(), sensorRead.getValue());
            }
        });
```

### 8.union

union与connect的区别：

1. Union 之前两个流的类型必须是一样，Connect 可以不一样，在之后的 coMap

中再去调整成为一样的。

 

2. Connect 只能操作两个流， Union 可以操作多个。

### 9.Rich Functions

“富函数”是 DataStream API 提供的一个函数类的接口， 所有 Flink 函数类都有其 Rich 版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。

-  RichMapFunction

-  RichFlatMapFunction

-  RichFilterFunction

- 	…

Rich Function 有一个生命周期的概念。典型的生命周期方法有：

-  open()方法是 rich function 的初始化方法，当一个算子例如 map 或者 filter

被调用之前 open()会被调用。

- close()方法是生命周期中的最后一个调用的方法，做一些清理工作。

-  getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函数执行的并行度，任务的名字，以及 state 状态\

```
public static class MyMapFunction extends RichMapFunction<SensorReading, Tuple2<Integer, String>> {
@Override
public Tuple2<Integer, String> map(SensorReading value) throws Exception {
return new Tuple2<>(getRuntimeContext().getIndexOfThisSubtask(), value.getId());
}


@Override
public void open(Configuration parameters) throws Exception { System.out.println("my map open");
// 以下可以做一些初始化工作，例如建立一个和 HDFS 的连接
}


@Override
public void close() throws Exception { System.out.println("my map close");
// 以下做一些清理工作，例如断开和 HDFS 的连接
}
}
```



## 二 source



## 三 sink





























